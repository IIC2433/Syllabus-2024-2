{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6487f3ea",
   "metadata": {},
   "source": [
    "Volvemos a cargar el dataset y una serie de otras cosas. \n",
    "\n",
    "Basado en este excelente [tutorial](https://keras.io/examples/graph/gnn_citations/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b416bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "citations = pd.read_csv(\"cora/cora.cites\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"target\", \"source\"],\n",
    ")\n",
    "\n",
    "column_names = [\"paper_id\"] + [f\"term_{idx}\" for idx in range(1433)] + [\"subject\"]\n",
    "papers = pd.read_csv(\"cora/cora.content\", sep=\"\\t\", header=None, names=column_names,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe3c4906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (1362, 1435)\n",
      "Test data shape: (1346, 1435)\n"
     ]
    }
   ],
   "source": [
    "class_values = sorted(papers[\"subject\"].unique())\n",
    "class_idx = {name: id for id, name in enumerate(class_values)}\n",
    "paper_idx = {name: idx for idx, name in enumerate(sorted(papers[\"paper_id\"].unique()))}\n",
    "\n",
    "papers[\"paper_id\"] = papers[\"paper_id\"].apply(lambda name: paper_idx[name])\n",
    "citations[\"source\"] = citations[\"source\"].apply(lambda name: paper_idx[name])\n",
    "citations[\"target\"] = citations[\"target\"].apply(lambda name: paper_idx[name])\n",
    "papers[\"subject\"] = papers[\"subject\"].apply(lambda value: class_idx[value])\n",
    "\n",
    "train_data, test_data = [], []\n",
    "\n",
    "for _, group_data in papers.groupby(\"subject\"):\n",
    "    # Select around 50% of the dataset for training.\n",
    "    random_selection = np.random.rand(len(group_data.index)) <= 0.5\n",
    "    train_data.append(group_data[random_selection])\n",
    "    test_data.append(group_data[~random_selection])\n",
    "\n",
    "train_data = pd.concat(train_data).sample(frac=1)\n",
    "test_data = pd.concat(test_data).sample(frac=1)\n",
    "\n",
    "print(\"Train data shape:\", train_data.shape)\n",
    "print(\"Test data shape:\", test_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "025c38f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>term_0</th>\n",
       "      <th>term_1</th>\n",
       "      <th>term_2</th>\n",
       "      <th>term_3</th>\n",
       "      <th>term_4</th>\n",
       "      <th>term_5</th>\n",
       "      <th>term_6</th>\n",
       "      <th>term_7</th>\n",
       "      <th>term_8</th>\n",
       "      <th>...</th>\n",
       "      <th>term_1424</th>\n",
       "      <th>term_1425</th>\n",
       "      <th>term_1426</th>\n",
       "      <th>term_1427</th>\n",
       "      <th>term_1428</th>\n",
       "      <th>term_1429</th>\n",
       "      <th>term_1430</th>\n",
       "      <th>term_1431</th>\n",
       "      <th>term_1432</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>1378</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2486</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>1274</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>196</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>1466</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1643</th>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>2647</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>1582</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2113</th>\n",
       "      <td>646</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1362 rows × 1435 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      paper_id  term_0  term_1  term_2  term_3  term_4  term_5  term_6  \\\n",
       "1272      1378       0       0       0       0       0       0       0   \n",
       "2486        51       0       0       0       0       0       0       0   \n",
       "367        667       0       0       0       0       0       0       0   \n",
       "668       1274       0       0       0       0       0       0       0   \n",
       "414        196       0       0       0       0       0       0       0   \n",
       "...        ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "950       1466       0       0       0       0       0       0       0   \n",
       "1643       135       0       0       1       0       0       0       0   \n",
       "222       2647       0       0       0       0       0       0       0   \n",
       "1231      1582       0       0       0       0       0       0       0   \n",
       "2113       646       0       0       0       0       0       0       0   \n",
       "\n",
       "      term_7  term_8  ...  term_1424  term_1425  term_1426  term_1427  \\\n",
       "1272       0       0  ...          0          0          0          0   \n",
       "2486       0       0  ...          0          0          0          0   \n",
       "367        0       0  ...          0          0          0          0   \n",
       "668        0       0  ...          1          0          0          0   \n",
       "414        0       0  ...          1          0          0          0   \n",
       "...      ...     ...  ...        ...        ...        ...        ...   \n",
       "950        0       0  ...          1          0          0          0   \n",
       "1643       0       0  ...          0          0          0          0   \n",
       "222        0       0  ...          0          0          0          0   \n",
       "1231       0       0  ...          0          0          0          0   \n",
       "2113       0       0  ...          0          0          0          0   \n",
       "\n",
       "      term_1428  term_1429  term_1430  term_1431  term_1432  subject  \n",
       "1272          0          0          0          0          0        2  \n",
       "2486          0          0          0          0          0        2  \n",
       "367           0          0          0          0          0        4  \n",
       "668           0          0          0          0          0        2  \n",
       "414           0          0          0          0          0        0  \n",
       "...         ...        ...        ...        ...        ...      ...  \n",
       "950           0          0          0          0          0        3  \n",
       "1643          0          0          0          0          0        4  \n",
       "222           0          0          0          0          0        0  \n",
       "1231          0          0          0          0          0        1  \n",
       "2113          0          0          0          0          0        6  \n",
       "\n",
       "[1362 rows x 1435 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aca93769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_MLP(capas_internas, dropout_rate, name=None):\n",
    "    capas = []\n",
    "\n",
    "    for capa_interna in capas_internas:\n",
    "        capas.append(layers.BatchNormalization())\n",
    "        capas.append(layers.Dropout(dropout_rate))\n",
    "        capas.append(layers.Dense(capa_interna, activation=tf.nn.gelu))\n",
    "\n",
    "    return keras.Sequential(capas, name=name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c45fca",
   "metadata": {},
   "source": [
    "## Manejo de datos específico para nuestras GNNs. \n",
    "\n",
    "Lo primero es que ahora las GNNs van a funcionar en base a las conexiones entro los papers (además de los features obviamente). La GNN se compila con la info del grado, por lo que el x_train y x_test solo deben tener los id de los nodos relevantes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34a6a66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(set(papers.columns) - {\"paper_id\", \"subject\"})\n",
    "num_features = len(feature_names)\n",
    "num_classes = len(class_idx)\n",
    "\n",
    "# Create train and test features as a numpy array.\n",
    "x_train = train_data[\"paper_id\"].to_numpy()\n",
    "x_test = test_data[\"paper_id\"].to_numpy()\n",
    "# Create train and test targets as a numpy array.\n",
    "y_train = train_data[\"subject\"]\n",
    "y_test = test_data[\"subject\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8defd0",
   "metadata": {},
   "source": [
    "El segundo paso es crear una matriz de adyacencia en formato numpy, que es lo que vamos a necesitar para pasarselo a tensorflow. Por razones de formato, es mejor usar una representación esparsa, en forma de lista de pares. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6424ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges shape: (2, 5429)\n",
      "Nodes shape: (2708, 1433)\n"
     ]
    }
   ],
   "source": [
    "#Matriz en forma de lista de pares\n",
    "edges = citations[[\"source\", \"target\"]].to_numpy().T\n",
    "\n",
    "#Codigo para agregar peso a cada arista, por ahora son puros 1s, todas valen lo mismo. \n",
    "edge_weights = tf.ones(shape=edges.shape[1])\n",
    "\n",
    "# Crear (en formato tensowrflow) los features para cada nodo.\n",
    "node_features = tf.cast(\n",
    "    papers.sort_values(\"paper_id\")[feature_names].to_numpy(), dtype=tf.dtypes.float32\n",
    ")\n",
    "\n",
    "#### juan: esto se puede simplificar\n",
    "# el grafo es la union de estas tres cosas\n",
    "graph_info = (node_features, edges, edge_weights)\n",
    "\n",
    "print(\"Edges shape:\", edges.shape)\n",
    "print(\"Nodes shape:\", node_features.shape)\n",
    "\n",
    "\n",
    "### Esto es muy importante. \n",
    "### El primer vector es la lista de los indices de los nodos source de edges, \n",
    "### El segundo vector es la lista de los indices de los nodos target\n",
    "\n",
    "node_indices, neighbour_indices = edges[0], edges[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8873591a",
   "metadata": {},
   "source": [
    "### Un modelo para una capa de la GNN\n",
    "\n",
    "Esta es la capa que va a hacer los pasos de agregación y update. \n",
    "Lamentablemente, la estructura de tensorflow nos obliga a definir estas operaciones, bastante complejas, como otras layers, por lo que procedemos a extender la clase *Layer*. Lo bueno es que funciona prácticamente igual que un *Model*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cd7185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNlayer(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        capas_internas = [32,32],\n",
    "        dropout_rate=0.2,\n",
    "        normalize=False,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(GNNlayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "        #Hay dos redes neuronales involucradas en una capa de GNN: la primera activación de los mensajes, \n",
    "        #    y el manejo del update. \n",
    "            \n",
    "        self.preprocesador = create_MLP(capas_internas, dropout_rate)\n",
    "\n",
    "        self.updater = create_MLP(capas_internas, dropout_rate)\n",
    "\n",
    "        \n",
    "    def prepare(self, node_repesentations, weights=None):\n",
    "        \n",
    "#        Esta funcion pasa los mensajes por una red neuronal simple, y aplica los pesos (si hay)\n",
    "        \n",
    "        messages = self.preprocesador(node_repesentations)\n",
    "        messages = messages * tf.expand_dims(weights, -1)\n",
    "        return messages\n",
    "\n",
    "    def aggregate(self, node_indices, neighbour_messages, node_repesentations):\n",
    "        \n",
    "        # Esta funcion agrega los mensajes de cada nodo, en forma de suma. \n",
    "        # recibo un vector node_indices, que es de largo [num_edges] y me dice los nodos origen de cada arista\n",
    "        # matriz neighbour_messages es de forma [num_edges, (neuronas_internas)], osea [num_edges, 32] en este codigo\n",
    "        # esta matriz tiene el mensaje de cada nodo que participa en la arista como nodo destino\n",
    "        # la matriz node_repesentations es de la forma [num_nodes, representation_dim], contiene información de \n",
    "        # los nodos del grafo. \n",
    "        \n",
    "        num_nodes = node_repesentations.shape[0]\n",
    "        \n",
    "                \n",
    "        #### La funcion unsorted_segment_sum me suma los mensajes de todos los indices iguales en node_indices y \n",
    "        #### los deja en el i-esimo lugar; con eso sumamos los ids. Si un nodo no tiene vecinos recibe un 0\n",
    "        \n",
    "        aggregated_message = tf.math.unsorted_segment_sum(\n",
    "            neighbour_messages, node_indices, num_segments=num_nodes\n",
    "            \n",
    "        )\n",
    "    \n",
    "        return aggregated_message\n",
    "\n",
    "    def update(self, node_repesentations, aggregated_messages):\n",
    "        \n",
    "        # Para combinar los mensajes con los features de cada nodo, concatenamos. \n",
    "        # Notar que a este punto tanto node_repesentations como aggregated_messages tienen forma \n",
    "        # [num_nodes, representation_dim]. Concat me los concatena. \n",
    "\n",
    "        h = tf.concat([node_repesentations, aggregated_messages], axis=1)\n",
    "        \n",
    "        # Y aplicamos unas capas no-lineales\n",
    "        \n",
    "        node_embeddings = self.updater(h)\n",
    "\n",
    "        return node_embeddings\n",
    "\n",
    "    def call(self, inputs):\n",
    "        ## Procesa los inputs para crear los embeddings. Siempre tenemos información de todo el grafo, \n",
    "        ## y operamos sobre todos los nodos en node_representations\n",
    "\n",
    "        node_repesentations, edges, edge_weights = inputs\n",
    "        \n",
    "        node_indices, neighbour_indices = edges[0], edges[1]\n",
    "        \n",
    "        # Lo primero es una lista de vectores en donde tomo cada id en neighbour_indices \n",
    "        # y lo reemplazo por la representación de ese id. \n",
    "        # El resultado es una lista que contiene, para cada arista, la representación del target de esa arista. \n",
    "        neighbour_repesentations = tf.gather(node_repesentations, neighbour_indices)\n",
    "\n",
    "        # Procesamos estos mensajes (posiblemente incluyendo pesos en aristas)\n",
    "        neighbour_messages = self.prepare(neighbour_repesentations, edge_weights)\n",
    "        \n",
    "        #Los agregamos\n",
    "        aggregated_messages = self.aggregate(\n",
    "            node_indices, neighbour_messages, node_repesentations\n",
    "        )\n",
    "        \n",
    "        # Y finalmente, el update. \n",
    "        return self.update(node_repesentations, aggregated_messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d707bb1",
   "metadata": {},
   "source": [
    "## Juntando todo en un clasificador\n",
    "\n",
    "Ahora si, definimos un modelo igual que la vez anterior, solo que ahora usa nuestra capa! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffabda49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNbasica(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        graph_info,\n",
    "        num_classes,\n",
    "        capas_internas = [32,32],\n",
    "        dropout_rate=0.2,\n",
    "        normalize=True,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \n",
    "        super(GNNbasica, self).__init__(*args, **kwargs)\n",
    "\n",
    "#LA GNN maneja información de todo el grafo, independiente del batch que procese. \n",
    "\n",
    "        node_features, edges, edge_weights = graph_info\n",
    "        self.node_features = node_features\n",
    "        self.edges = edges\n",
    "        self.edge_weights = edge_weights\n",
    "        \n",
    "        #normalizar\n",
    "        self.edge_weights = self.edge_weights / tf.math.reduce_sum(self.edge_weights)\n",
    "\n",
    "#Las layers básicas: una capa para preprocesar todo        \n",
    "        self.preprocesar = create_MLP(capas_internas, dropout_rate, name=\"preprocesado\")\n",
    "    \n",
    "# dos capas de paso de mensjaes \n",
    "\n",
    "        self.capa1 = GNNlayer(\n",
    "            capas_internas,\n",
    "            dropout_rate,\n",
    "            name=\"capa1\",\n",
    "        )\n",
    "        # Create the second GraphConv layer.\n",
    "        self.capa2 = GNNlayer(\n",
    "            capas_internas,\n",
    "            dropout_rate,\n",
    "            name=\"capa2\",\n",
    "        )\n",
    "\n",
    "        self.capaMA = GNNlayer(\n",
    "            capas_internas,\n",
    "            dropout_rate,\n",
    "            name=\"capaMA\",\n",
    "        )\n",
    "        \n",
    "        # Create a postprocess layer.\n",
    "        self.postprocess = create_MLP(capas_internas, dropout_rate, name=\"postprocess\")\n",
    "        # Create a compute logits layer.\n",
    "        self.clas = layers.Dense(units=num_classes, name=\"logits\")\n",
    "\n",
    "        \n",
    "    def call(self, batch_indices):\n",
    "        \n",
    "        #### Capa de preprocesado de features, para bajar la dimensionalidad\n",
    "        \n",
    "        nodos_preprocesados = self.preprocesar(self.node_features)\n",
    "        \n",
    "        #### Estos nodos preprocesados pasan por la capa1, la que agrega los mensajes de sus vecinos. \n",
    "            \n",
    "        paso_mens1 = self.capa1((nodos_preprocesados,self.edges, self.edge_weights))\n",
    "        \n",
    "        skip1 = nodos_preprocesados + paso_mens1 \n",
    "        \n",
    "        paso_mens2 = self.capa2((skip1,self.edges, self.edge_weights))\n",
    "        \n",
    "        skip2 = paso_mens2 + skip1\n",
    "\n",
    "        MA = self.capaMA(skip2)\n",
    "\n",
    "        ##### Postprocesado y llegar a la categoría del nodo\n",
    "        postprocesado = self.postprocess(MA)\n",
    "\n",
    "        ##### Volvemos a poner los embeddings en el orden que demandaba segun el batch\n",
    "        node_embeddings = tf.gather(postprocesado, batch_indices)\n",
    "\n",
    "        # Readout para llegar a las categorias\n",
    "        \n",
    "        return self.clas(node_embeddings)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9aacaa74",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m GNN \u001b[38;5;241m=\u001b[39m \u001b[43mGNNbasica\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapas_internas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgnn_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#print(GNN([1, 10, 100]))\u001b[39;00m\n\u001b[1;32m     11\u001b[0m GNN\u001b[38;5;241m.\u001b[39msummary()\n",
      "Cell \u001b[0;32mIn[17], line 26\u001b[0m, in \u001b[0;36mGNNbasica.__init__\u001b[0;34m(self, graph_info, num_classes, capas_internas, dropout_rate, normalize, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_weights \u001b[38;5;241m/\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mreduce_sum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_weights)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#Las layers básicas: una capa para preprocesar todo        \u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocesar \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_MLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcapas_internas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpreprocesado\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# dos capas de paso de mensjaes \u001b[39;00m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcapa1 \u001b[38;5;241m=\u001b[39m GNNlayer(\n\u001b[1;32m     31\u001b[0m             capas_internas,\n\u001b[1;32m     32\u001b[0m             dropout_rate,\n\u001b[1;32m     33\u001b[0m             name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapa1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     34\u001b[0m         )\n",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m, in \u001b[0;36mcreate_MLP\u001b[0;34m(hidden_layers, dropout_rate, name)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m hidden_layers:\n\u001b[1;32m      6\u001b[0m     num_layer \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 7\u001b[0m     neurons_in \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      8\u001b[0m     neurons_out \u001b[38;5;241m=\u001b[39m layer[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m.\u001b[39madd_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatchnorm\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(num_layer), nn\u001b[38;5;241m.\u001b[39mBatchNorm1d(neurons_in))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\n",
    "GNN = GNNbasica(\n",
    "    graph_info=graph_info,\n",
    "    num_classes=7,\n",
    "    capas_internas=[32,32],\n",
    "    dropout_rate=0.2,\n",
    "    name=\"gnn_model\",\n",
    ")\n",
    "\n",
    "#print(GNN([1, 10, 100]))\n",
    "\n",
    "GNN.summary()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9ff100a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - acc: 0.2394 - loss: 1.9498 - val_acc: 0.2330 - val_loss: 1.8574\n",
      "Epoch 2/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - acc: 0.4006 - loss: 1.7877 - val_acc: 0.4078 - val_loss: 1.6851\n",
      "Epoch 3/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - acc: 0.6111 - loss: 1.4139 - val_acc: 0.4709 - val_loss: 1.5596\n",
      "Epoch 4/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - acc: 0.7341 - loss: 0.7789 - val_acc: 0.5146 - val_loss: 1.9149\n",
      "Epoch 5/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - acc: 0.7868 - loss: 0.6284 - val_acc: 0.6068 - val_loss: 1.3117\n",
      "Epoch 6/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - acc: 0.8623 - loss: 0.3690 - val_acc: 0.6505 - val_loss: 1.1667\n",
      "Epoch 7/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - acc: 0.9195 - loss: 0.2499 - val_acc: 0.6748 - val_loss: 1.4522\n",
      "Epoch 8/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - acc: 0.9618 - loss: 0.1184 - val_acc: 0.6748 - val_loss: 1.7242\n",
      "Epoch 9/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - acc: 0.9827 - loss: 0.0579 - val_acc: 0.6650 - val_loss: 1.8366\n",
      "Epoch 10/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - acc: 0.9914 - loss: 0.0383 - val_acc: 0.6650 - val_loss: 2.1839\n",
      "Epoch 11/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - acc: 0.9919 - loss: 0.0286 - val_acc: 0.6553 - val_loss: 2.1459\n",
      "Epoch 12/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - acc: 0.9897 - loss: 0.0254 - val_acc: 0.6796 - val_loss: 2.3867\n",
      "Epoch 13/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - acc: 0.9933 - loss: 0.0226 - val_acc: 0.6845 - val_loss: 2.5715\n",
      "Epoch 14/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - acc: 0.9976 - loss: 0.0127 - val_acc: 0.6796 - val_loss: 2.6652\n",
      "Epoch 15/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - acc: 0.9997 - loss: 0.0053 - val_acc: 0.6699 - val_loss: 2.6341\n",
      "Epoch 16/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - acc: 0.9982 - loss: 0.0046 - val_acc: 0.6602 - val_loss: 2.7054\n",
      "Epoch 17/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - acc: 0.9993 - loss: 0.0041 - val_acc: 0.6699 - val_loss: 2.7393\n",
      "Epoch 18/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - acc: 0.9996 - loss: 0.0026 - val_acc: 0.6748 - val_loss: 2.7684\n",
      "Epoch 19/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - acc: 0.9986 - loss: 0.0040 - val_acc: 0.6505 - val_loss: 2.8672\n",
      "Epoch 20/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - acc: 0.9993 - loss: 0.0027 - val_acc: 0.6699 - val_loss: 2.7937\n",
      "Epoch 21/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - acc: 0.9981 - loss: 0.0029 - val_acc: 0.6796 - val_loss: 2.7754\n",
      "Epoch 22/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - acc: 0.9990 - loss: 0.0016 - val_acc: 0.6699 - val_loss: 2.9829\n",
      "Epoch 23/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - acc: 0.9997 - loss: 0.0014 - val_acc: 0.6845 - val_loss: 2.8797\n",
      "Epoch 24/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - acc: 0.9990 - loss: 0.0028 - val_acc: 0.6845 - val_loss: 3.0599\n",
      "Epoch 25/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - acc: 1.0000 - loss: 8.9705e-04 - val_acc: 0.6796 - val_loss: 3.0759\n",
      "Epoch 26/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - acc: 1.0000 - loss: 0.0013 - val_acc: 0.6893 - val_loss: 3.1091\n",
      "Epoch 27/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - acc: 1.0000 - loss: 4.2128e-04 - val_acc: 0.6893 - val_loss: 3.1627\n",
      "Epoch 28/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - acc: 1.0000 - loss: 2.9073e-04 - val_acc: 0.6845 - val_loss: 3.1919\n",
      "Epoch 29/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - acc: 1.0000 - loss: 3.8457e-04 - val_acc: 0.6845 - val_loss: 3.2072\n",
      "Epoch 30/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - acc: 1.0000 - loss: 1.7652e-04 - val_acc: 0.6845 - val_loss: 3.2399\n",
      "Epoch 31/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - acc: 1.0000 - loss: 1.7169e-04 - val_acc: 0.6893 - val_loss: 3.2651\n",
      "Epoch 32/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - acc: 1.0000 - loss: 9.5234e-05 - val_acc: 0.6942 - val_loss: 3.2917\n",
      "Epoch 33/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - acc: 1.0000 - loss: 9.5720e-05 - val_acc: 0.6796 - val_loss: 3.3169\n",
      "Epoch 34/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - acc: 1.0000 - loss: 7.5476e-05 - val_acc: 0.6845 - val_loss: 3.3347\n",
      "Epoch 35/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - acc: 1.0000 - loss: 8.9419e-05 - val_acc: 0.6845 - val_loss: 3.3566\n",
      "Epoch 36/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - acc: 1.0000 - loss: 5.4563e-05 - val_acc: 0.6845 - val_loss: 3.3722\n",
      "Epoch 37/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - acc: 1.0000 - loss: 6.3079e-05 - val_acc: 0.6845 - val_loss: 3.3884\n",
      "Epoch 38/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - acc: 1.0000 - loss: 4.4148e-05 - val_acc: 0.6845 - val_loss: 3.4020\n",
      "Epoch 39/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - acc: 1.0000 - loss: 4.9595e-05 - val_acc: 0.6845 - val_loss: 3.4204\n",
      "Epoch 40/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - acc: 1.0000 - loss: 4.6671e-05 - val_acc: 0.6845 - val_loss: 3.4361\n",
      "Epoch 41/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - acc: 1.0000 - loss: 6.0072e-05 - val_acc: 0.6845 - val_loss: 3.4540\n",
      "Epoch 42/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - acc: 1.0000 - loss: 4.2416e-05 - val_acc: 0.6845 - val_loss: 3.4670\n",
      "Epoch 43/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - acc: 1.0000 - loss: 4.0703e-05 - val_acc: 0.6845 - val_loss: 3.4759\n",
      "Epoch 44/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - acc: 1.0000 - loss: 4.1871e-05 - val_acc: 0.6845 - val_loss: 3.4850\n",
      "Epoch 45/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - acc: 1.0000 - loss: 4.5402e-05 - val_acc: 0.6845 - val_loss: 3.4947\n",
      "Epoch 46/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - acc: 1.0000 - loss: 5.2645e-05 - val_acc: 0.6845 - val_loss: 3.5038\n",
      "Epoch 47/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - acc: 1.0000 - loss: 4.7873e-05 - val_acc: 0.6845 - val_loss: 3.5125\n",
      "Epoch 48/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - acc: 1.0000 - loss: 4.5870e-05 - val_acc: 0.6845 - val_loss: 3.5212\n",
      "Epoch 49/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - acc: 1.0000 - loss: 3.0613e-05 - val_acc: 0.6845 - val_loss: 3.5294\n",
      "Epoch 50/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - acc: 1.0000 - loss: 2.8981e-05 - val_acc: 0.6845 - val_loss: 3.5368\n",
      "Epoch 51/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - acc: 1.0000 - loss: 3.1610e-05 - val_acc: 0.6845 - val_loss: 3.5455\n",
      "Epoch 52/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - acc: 1.0000 - loss: 3.8046e-05 - val_acc: 0.6845 - val_loss: 3.5553\n",
      "Epoch 53/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - acc: 1.0000 - loss: 2.5262e-05 - val_acc: 0.6845 - val_loss: 3.5639\n",
      "Epoch 54/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - acc: 1.0000 - loss: 2.9708e-05 - val_acc: 0.6845 - val_loss: 3.5720\n",
      "Epoch 55/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - acc: 1.0000 - loss: 2.3772e-05 - val_acc: 0.6845 - val_loss: 3.5800\n",
      "Epoch 56/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - acc: 1.0000 - loss: 2.5866e-05 - val_acc: 0.6845 - val_loss: 3.5882\n",
      "Epoch 57/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - acc: 1.0000 - loss: 2.5551e-05 - val_acc: 0.6845 - val_loss: 3.5962\n",
      "Epoch 58/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - acc: 1.0000 - loss: 2.7389e-05 - val_acc: 0.6845 - val_loss: 3.6039\n",
      "Epoch 59/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - acc: 1.0000 - loss: 3.0774e-05 - val_acc: 0.6845 - val_loss: 3.6113\n",
      "Epoch 60/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - acc: 1.0000 - loss: 2.0945e-05 - val_acc: 0.6845 - val_loss: 3.6167\n",
      "Epoch 61/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - acc: 1.0000 - loss: 2.8642e-05 - val_acc: 0.6845 - val_loss: 3.6238\n",
      "Epoch 62/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - acc: 1.0000 - loss: 1.9048e-05 - val_acc: 0.6845 - val_loss: 3.6299\n",
      "Epoch 63/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - acc: 1.0000 - loss: 2.2191e-05 - val_acc: 0.6893 - val_loss: 3.6393\n",
      "Epoch 64/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - acc: 1.0000 - loss: 1.9065e-05 - val_acc: 0.6893 - val_loss: 3.6470\n",
      "Epoch 65/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - acc: 1.0000 - loss: 2.1529e-05 - val_acc: 0.6893 - val_loss: 3.6548\n",
      "Epoch 66/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - acc: 1.0000 - loss: 1.4198e-05 - val_acc: 0.6893 - val_loss: 3.6610\n",
      "Epoch 67/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - acc: 1.0000 - loss: 2.4856e-05 - val_acc: 0.6893 - val_loss: 3.6700\n",
      "Epoch 68/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - acc: 1.0000 - loss: 1.5950e-05 - val_acc: 0.6893 - val_loss: 3.6763\n",
      "Epoch 69/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - acc: 1.0000 - loss: 1.9333e-05 - val_acc: 0.6893 - val_loss: 3.6831\n",
      "Epoch 70/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - acc: 1.0000 - loss: 1.7453e-05 - val_acc: 0.6893 - val_loss: 3.6888\n",
      "Epoch 71/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - acc: 1.0000 - loss: 1.4091e-05 - val_acc: 0.6893 - val_loss: 3.6942\n",
      "Epoch 72/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - acc: 1.0000 - loss: 1.3989e-05 - val_acc: 0.6893 - val_loss: 3.6998\n",
      "Epoch 73/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - acc: 1.0000 - loss: 1.2904e-05 - val_acc: 0.6893 - val_loss: 3.7049\n",
      "Epoch 74/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - acc: 1.0000 - loss: 1.4616e-05 - val_acc: 0.6893 - val_loss: 3.7104\n",
      "Epoch 75/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - acc: 1.0000 - loss: 1.1741e-05 - val_acc: 0.6893 - val_loss: 3.7176\n",
      "Epoch 76/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - acc: 1.0000 - loss: 1.4859e-05 - val_acc: 0.6893 - val_loss: 3.7252\n",
      "Epoch 77/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - acc: 1.0000 - loss: 1.0894e-05 - val_acc: 0.6893 - val_loss: 3.7315\n",
      "Epoch 78/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - acc: 1.0000 - loss: 1.4972e-05 - val_acc: 0.6893 - val_loss: 3.7393\n",
      "Epoch 79/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - acc: 1.0000 - loss: 1.3810e-05 - val_acc: 0.6893 - val_loss: 3.7450\n",
      "Epoch 80/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - acc: 1.0000 - loss: 1.2668e-05 - val_acc: 0.6893 - val_loss: 3.7502\n",
      "Epoch 81/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - acc: 1.0000 - loss: 1.1741e-05 - val_acc: 0.6893 - val_loss: 3.7557\n",
      "Epoch 82/300\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - acc: 1.0000 - loss: 1.0615e-05 - val_acc: 0.6893 - val_loss: 3.7603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 86ms/step - loss: 0.0316 - acc: 0.9912 - val_loss: 1.1568 - val_acc: 0.8557\n",
      "Epoch 250/300\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.0266 - acc: 0.9947 - val_loss: 1.0264 - val_acc: 0.8657\n",
      "Epoch 251/300\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.0175 - acc: 0.9947 - val_loss: 0.9971 - val_acc: 0.8507\n",
      "Epoch 252/300\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.0294 - acc: 0.9921 - val_loss: 1.0891 - val_acc: 0.8159\n",
      "Epoch 253/300\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.0229 - acc: 0.9938 - val_loss: 1.1051 - val_acc: 0.8109\n",
      "Epoch 254/300\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.0272 - acc: 0.9930 - val_loss: 1.1562 - val_acc: 0.8209\n",
      "Epoch 255/300\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.0187 - acc: 0.9930 - val_loss: 1.1787 - val_acc: 0.8308\n",
      "Epoch 256/300\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.0246 - acc: 0.9921 - val_loss: 1.1327 - val_acc: 0.8308\n",
      "Epoch 257/300\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.0253 - acc: 0.9930 - val_loss: 1.0916 - val_acc: 0.8458\n",
      "Epoch 258/300\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.0117 - acc: 0.9965 - val_loss: 1.1155 - val_acc: 0.8408\n",
      "Epoch 259/300\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.0231 - acc: 0.9930 - val_loss: 1.0298 - val_acc: 0.8607\n",
      "Epoch 260/300\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.0200 - acc: 0.9956 - val_loss: 0.9970 - val_acc: 0.8557\n",
      "Epoch 261/300\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.0259 - acc: 0.9895 - val_loss: 0.9855 - val_acc: 0.8806\n",
      "Epoch 262/300\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.0189 - acc: 0.9947 - val_loss: 1.0686 - val_acc: 0.8557\n",
      "Epoch 263/300\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.0219 - acc: 0.9938 - val_loss: 1.1415 - val_acc: 0.8458\n",
      "Epoch 264/300\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.0275 - acc: 0.9956 - val_loss: 1.1308 - val_acc: 0.8507\n",
      "Epoch 265/300\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.0068 - acc: 0.9982 - val_loss: 1.1025 - val_acc: 0.8557\n",
      "Epoch 266/300\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.0157 - acc: 0.9921 - val_loss: 1.0004 - val_acc: 0.8706\n",
      "Epoch 267/300\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.0207 - acc: 0.9912 - val_loss: 0.9768 - val_acc: 0.8706\n",
      "Epoch 268/300\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.0158 - acc: 0.9938 - val_loss: 0.9882 - val_acc: 0.8657\n",
      "Epoch 269/300\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.0214 - acc: 0.9947 - val_loss: 1.0717 - val_acc: 0.8657\n",
      "Epoch 270/300\n",
      "5/5 [==============================] - 0s 101ms/step - loss: 0.0178 - acc: 0.9965 - val_loss: 1.1673 - val_acc: 0.8507\n",
      "Epoch 271/300\n",
      "5/5 [==============================] - 1s 132ms/step - loss: 0.0320 - acc: 0.9930 - val_loss: 1.1204 - val_acc: 0.8657\n",
      "Epoch 272/300\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.0288 - acc: 0.9930 - val_loss: 1.0678 - val_acc: 0.8557\n",
      "Epoch 273/300\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.0284 - acc: 0.9947 - val_loss: 1.1179 - val_acc: 0.8408\n",
      "Epoch 274/300\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.0326 - acc: 0.9912 - val_loss: 1.0343 - val_acc: 0.8607\n",
      "Epoch 275/300\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.0155 - acc: 0.9947 - val_loss: 1.0123 - val_acc: 0.8657\n",
      "Epoch 276/300\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.0150 - acc: 0.9947 - val_loss: 1.0258 - val_acc: 0.8657\n",
      "Epoch 277/300\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.0240 - acc: 0.9938 - val_loss: 1.0187 - val_acc: 0.8706\n",
      "Epoch 278/300\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.0136 - acc: 0.9938 - val_loss: 0.9958 - val_acc: 0.8806\n",
      "Epoch 279/300\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.0188 - acc: 0.9938 - val_loss: 0.9996 - val_acc: 0.8706\n",
      "Epoch 280/300\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.0198 - acc: 0.9912 - val_loss: 1.0748 - val_acc: 0.8557\n",
      "Epoch 281/300\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.0255 - acc: 0.9938 - val_loss: 1.0829 - val_acc: 0.8507\n",
      "Epoch 282/300\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.0390 - acc: 0.9877 - val_loss: 1.0464 - val_acc: 0.8557\n",
      "Epoch 283/300\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.0142 - acc: 0.9974 - val_loss: 1.0142 - val_acc: 0.8607\n",
      "Epoch 284/300\n",
      "5/5 [==============================] - 0s 102ms/step - loss: 0.0229 - acc: 0.9912 - val_loss: 1.0233 - val_acc: 0.8557\n",
      "Epoch 285/300\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.0198 - acc: 0.9947 - val_loss: 1.0462 - val_acc: 0.8507\n",
      "Epoch 286/300\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.0121 - acc: 0.9974 - val_loss: 1.0370 - val_acc: 0.8607\n",
      "Epoch 287/300\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.0241 - acc: 0.9930 - val_loss: 1.0126 - val_acc: 0.8756\n",
      "Epoch 288/300\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.0166 - acc: 0.9947 - val_loss: 1.0257 - val_acc: 0.8706\n",
      "Epoch 289/300\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.0065 - acc: 0.9974 - val_loss: 1.0784 - val_acc: 0.8607\n",
      "Epoch 290/300\n",
      "5/5 [==============================] - 1s 104ms/step - loss: 0.0164 - acc: 0.9930 - val_loss: 1.0758 - val_acc: 0.8657\n",
      "Epoch 291/300\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.0069 - acc: 0.9974 - val_loss: 1.0719 - val_acc: 0.8706\n",
      "Epoch 292/300\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.0163 - acc: 0.9921 - val_loss: 1.0763 - val_acc: 0.8756\n",
      "Epoch 293/300\n",
      "5/5 [==============================] - 1s 106ms/step - loss: 0.0072 - acc: 0.9991 - val_loss: 1.0681 - val_acc: 0.8856\n",
      "Epoch 294/300\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.0147 - acc: 0.9956 - val_loss: 1.1146 - val_acc: 0.8756\n",
      "Epoch 295/300\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.0143 - acc: 0.9938 - val_loss: 1.1679 - val_acc: 0.8657\n",
      "Epoch 296/300\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.0205 - acc: 0.9921 - val_loss: 1.1941 - val_acc: 0.8657\n",
      "Epoch 297/300\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.0287 - acc: 0.9921 - val_loss: 1.0282 - val_acc: 0.8806\n",
      "Epoch 298/300\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.0299 - acc: 0.9921 - val_loss: 1.0522 - val_acc: 0.8607\n",
      "Epoch 299/300\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.0123 - acc: 0.9965 - val_loss: 1.0440 - val_acc: 0.8557\n",
      "Epoch 300/300\n",
      "5/5 [==============================] - 1s 101ms/step - loss: 0.0083 - acc: 0.9965 - val_loss: 0.9985 - val_acc: 0.8607\n"
     ]
    }
   ],
   "source": [
    "GNN.compile(\n",
    "        optimizer=keras.optimizers.Adam(0.01),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
    "    )\n",
    "\n",
    "# Create an early stopping callback.\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_acc\", patience=50, restore_best_weights=True\n",
    "    )\n",
    "\n",
    "# A la GNN hay que darlos los ids de los nodos \n",
    "\n",
    "\n",
    "    # Fit the model.\n",
    "history = GNN.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        epochs=300,\n",
    "        batch_size=256,\n",
    "        validation_split=0.15,\n",
    "        callbacks=[early_stopping],\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6865b392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.6944 - loss: 3.4674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.54608416557312, 0.6878742575645447]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = test_data.paper_id.to_numpy()\n",
    "GNN.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed546713",
   "metadata": {},
   "source": [
    "Mucho mejor! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4382aa7b-7e34-4fb9-8388-a5e85228f0ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gnn_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gnn_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ preprocesado (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)       │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2708</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">52,804</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ capa1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GNNlayer</span>)                │ ?                      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,888</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ capa2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GNNlayer</span>)                │ ?                      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,888</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ postprocess (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2708</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,368</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ logits (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">231</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ preprocesado (\u001b[38;5;33mSequential\u001b[0m)       │ (\u001b[38;5;34m2708\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m52,804\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ capa1 (\u001b[38;5;33mGNNlayer\u001b[0m)                │ ?                      │         \u001b[38;5;34m5,888\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ capa2 (\u001b[38;5;33mGNNlayer\u001b[0m)                │ ?                      │         \u001b[38;5;34m5,888\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ postprocess (\u001b[38;5;33mSequential\u001b[0m)        │ (\u001b[38;5;34m2708\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,368\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ logits (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │           \u001b[38;5;34m231\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">194,143</span> (758.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m194,143\u001b[0m (758.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">63,481</span> (247.97 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m63,481\u001b[0m (247.97 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,698</span> (14.45 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,698\u001b[0m (14.45 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">126,964</span> (495.96 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m126,964\u001b[0m (495.96 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a492e29",
   "metadata": {},
   "source": [
    "# Actividades sugeridas\n",
    "\n",
    "### Averigua sobre GRUs, y lee el paper de Gated Graph Sequence Neural Networks. Implementa un clasificador de acuerdo con esa tecnología. Puedes tambien ver una implementación [aquí](https://keras.io/examples/graph/gnn_citations/).  \n",
    "\n",
    "### Averigua sobre atención en grafos, y lee el paper de Graph Attention Networks. Implementa un clasificador de acuerdo con esa tecnología. Puedes tambien ver una implementación [aquí](https://keras.io/examples/graph/gat_node_classification/).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bf23c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
